{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot, train, test, cls, voc = data_cutter(amazon, imdb, yelp)\n",
    "\n",
    "# good = [word for word in voc if '1' in voc[word][\"class\"]]\n",
    "# bad = [word for word in voc if '0' in voc[word][\"class\"]]\n",
    "\n",
    "# unq_good = [word for word in good if word not in bad]\n",
    "# unq_bad = [word for word in bad if word not in good]\n",
    "\n",
    "# both = [word for word in voc if word in good and word in bad]\n",
    "\n",
    "# # for word in both:\n",
    "# #     print(\"{}\\t-> {}\".format(word, voc[word]))\n",
    "\n",
    "# print(cls)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import *\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, *files, train_partition=0.8, quiet=True, shuffle=True, trim=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param files:\n",
    "        \"\"\"\n",
    "        self.files = files\n",
    "        self.train_partition = train_partition\n",
    "        self.quiet = quiet\n",
    "        self.shuffle = shuffle\n",
    "        self.trim = trim\n",
    "        self.total_data, \\\n",
    "        self.training_data, \\\n",
    "        self.test_data, \\\n",
    "        self.doc_classes, \\\n",
    "        self.vocabulary = self.data_cutter()\n",
    "        if self.trim:\n",
    "            self.trim_common()\n",
    "        self.voc_classes = self.count_class()\n",
    "        self.log_priors = self.log_prior()\n",
    "        self.log_likelihood()\n",
    "\n",
    "    def data_cutter(self):\n",
    "\n",
    "        # code to cut data\n",
    "        files = self.files\n",
    "        total_data = []\n",
    "        training_data = []\n",
    "        test_data = []\n",
    "        voc_classes = {}\n",
    "        doc_classes = {}\n",
    "        vocabulary = {}\n",
    "\n",
    "        word_count = 0\n",
    "\n",
    "        punct = \"!@#$%^&*()_'''+=,./;[]\\<>?:{}|-```~1234567890\"\n",
    "\n",
    "        for f in files:\n",
    "            data = open(f, \"r\")\n",
    "\n",
    "            c = 0\n",
    "            w = 0\n",
    "\n",
    "            for line in data:\n",
    "                t_line = line \\\n",
    "                    .rstrip('\\n') \\\n",
    "                    .split('\\t')\n",
    "\n",
    "                label = int(t_line[1])\n",
    "                sentence = t_line[0].lower()\n",
    "\n",
    "                for sym in punct:\n",
    "                    sentence = sentence.replace(sym, '')\n",
    "\n",
    "                if label not in doc_classes:\n",
    "                    doc_classes[label] = 0\n",
    "\n",
    "                # split words in each sentence.\n",
    "                # TODO: tokenize and remove filler words\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "\n",
    "                doc = (label, words)\n",
    "\n",
    "                total_data.append(doc)\n",
    "\n",
    "                w += len(words)\n",
    "            # TODO: create shuffle trigger\n",
    "            if self.shuffle:\n",
    "                random.shuffle(total_data)\n",
    "\n",
    "            # TODO: create portioning variable\n",
    "            training_data = total_data[0:int(self.train_partition * len(total_data))]\n",
    "            test_data = total_data[int(self.train_partition * len(total_data)):]\n",
    "\n",
    "            c += 1\n",
    "            if not self.quiet:\n",
    "                print('{} finished on line {} with {} words'\n",
    "                      '\\n==========================\\n'.format(f, c, w))\n",
    "            data.close()\n",
    "\n",
    "            # exit file loop\n",
    "\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        for doc in training_data:\n",
    "\n",
    "            label = doc[0]\n",
    "            words = doc[1]\n",
    "\n",
    "            if label not in doc_classes:\n",
    "                doc_classes[label] = 1\n",
    "            else:\n",
    "                doc_classes[label] += 1\n",
    "\n",
    "            for word in words:\n",
    "                if word in stopWords:\n",
    "                    pass\n",
    "                else:\n",
    "                    if word not in vocabulary:\n",
    "                        # initialising word in vocabulary\n",
    "                        vocabulary[word] = {\"frequency\": {l: 0 for l in doc_classes}, \"class\": [label]}\n",
    "                        vocabulary[word][\"frequency\"][label] += 1\n",
    "                    else:\n",
    "                        if label in vocabulary[word][\"class\"]:\n",
    "                            vocabulary[word][\"frequency\"][label] += 1\n",
    "\n",
    "                        else:\n",
    "                            vocabulary[word][\"class\"].append(label)\n",
    "                            vocabulary[word][\"frequency\"][label] = 1\n",
    "\n",
    "                    # if label not in voc_classes:\n",
    "                    #     voc_classes[label] = 1\n",
    "                    # else:\n",
    "                    #     voc_classes[label] += 1\n",
    "\n",
    "                    word_count += 1\n",
    "\n",
    "        # voc_classes = self.count_class()\n",
    "\n",
    "        #\n",
    "        #     c += 1\n",
    "        #\n",
    "        # print('{} finished on line {} with {} words'\n",
    "        #       '\\n==========================\\n'.format(f, c, w))\n",
    "        if not self.quiet:\n",
    "            print(\n",
    "                \"=====Summary=====\\n\"\n",
    "                \"Unique words:\\t -> {}\\n\"\n",
    "                \"Total words:\\t -> {}\\n\"\n",
    "                \"Total data:\\t -> {}\\n\"\n",
    "                \"Train data:\\t -> {}\\n\"\n",
    "                \"Test data:\\t -> {}\\n\"\n",
    "                    .format(len(vocabulary),\n",
    "                            word_count,\n",
    "                            len(total_data),\n",
    "                            len(training_data),\n",
    "                            len(test_data)))\n",
    "\n",
    "        return total_data, training_data, test_data, doc_classes, vocabulary\n",
    "\n",
    "    # def vocabulary(self):\n",
    "    #     return self.vocabulary\n",
    "    #\n",
    "    # def word(self, word):\n",
    "    #     return self.vocabulary[word]\n",
    "\n",
    "    def count_class(self):\n",
    "        nc = {l: 0 for l in self.doc_classes}\n",
    "        for word, data in self.vocabulary.items():\n",
    "            for cls in self.doc_classes:\n",
    "                nc[cls] += data['frequency'][cls]\n",
    "        return nc\n",
    "\n",
    "    def log_likelihood(self):\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "            self.vocabulary[word][\"probability\"] = {}\n",
    "            for cls, wfr in self.vocabulary[word][\"frequency\"].items():\n",
    "                cf = self.voc_classes[cls]\n",
    "\n",
    "                p = log10((wfr + 1) / (cf + 1))\n",
    "\n",
    "                self.vocabulary[word][\"probability\"][cls] = p\n",
    "        if not self.quiet:\n",
    "            print(\"Log probabilities calculated\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def log_prior(self):\n",
    "        log_priors = {}\n",
    "        n_doc = len(self.training_data)\n",
    "        for cls, n_c in self.doc_classes.items():\n",
    "            pc = log10(n_c / n_doc)\n",
    "            log_priors[cls] = pc\n",
    "\n",
    "        return log_priors\n",
    "\n",
    "    def nb_classifier(self, doc):\n",
    "        sum_c = {}\n",
    "        for cls, lp in self.log_priors.items():\n",
    "            # print('{} -> {}'.format(cls, lp))\n",
    "            sum_c[cls] = lp\n",
    "            for word in doc:\n",
    "                if word in self.vocabulary:\n",
    "                    sum_c[cls] += self.vocabulary[word]['probability'][cls]\n",
    "\n",
    "        return max(zip(sum_c.values(), sum_c.keys()))\n",
    "\n",
    "    def test(self):\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        for doc in self.test_data:\n",
    "            words = doc[1]\n",
    "            label = doc[0]\n",
    "\n",
    "            result = self.nb_classifier(words)\n",
    "            if result[1] == label:\n",
    "                score += 1\n",
    "            word_count += 1\n",
    "\n",
    "        return (score / word_count) * 100\n",
    "\n",
    "    def trim_common(self):\n",
    "        # remove words with similar frequencies\n",
    "        del_w = []\n",
    "        for w, fr in self.vocabulary.items():\n",
    "            if len(fr['class']) > 1:\n",
    "                if sum(fr['frequency'].values()) > 10:\n",
    "                    tot = sum(fr['frequency'].values())\n",
    "                    perc = []\n",
    "                    for c, f in fr['frequency'].items():\n",
    "                        perc.append((f / tot) * 100)\n",
    "                    dif = abs(perc[0] - perc[1])\n",
    "                    if dif < 15:\n",
    "                        del_w.append(w)\n",
    "\n",
    "        for w in del_w:\n",
    "            del self.vocabulary[w]\n",
    "\n",
    "        if not self.quiet:\n",
    "            print('{} words removed from vocabulary'.format(len(del_w)))\n",
    "        del del_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells_labelled.txt finished on line 1000 with 10202 words\n",
      "==========================\n",
      "\n",
      "imdb_labelled.txt finished on line 1000 with 14300 words\n",
      "==========================\n",
      "\n",
      "yelp_labelled.txt finished on line 1000 with 10848 words\n",
      "==========================\n",
      "\n",
      "=====Summary=====\n",
      "Unique words:\t -> 3416\n",
      "Total words:\t -> 28615\n",
      "Total data:\t -> 3000\n",
      "Train data:\t -> 2400\n",
      "Test data:\t -> 600\n",
      "\n",
      "38 words removed from vocabulary\n",
      "Log probabilities calculated\n",
      "82.0\n"
     ]
    }
   ],
   "source": [
    "# unit test classifier \n",
    "from util import NB_DataHandler\n",
    "\n",
    "\n",
    "\n",
    "amazon = \"amazon_cells_labelled.txt\"\n",
    "imdb = \"imdb_labelled.txt\"\n",
    "yelp = \"yelp_labelled.txt\"\n",
    "\n",
    "v2 = NB_DataHandler(amazon, imdb, yelp, quiet=False)\n",
    "res = v2.test()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy over several runs\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "runs = 100\n",
    "vals = []\n",
    "start = time.time()\n",
    "for i in range(1, runs+1):\n",
    "    if i%10 == 0:\n",
    "        t = time.time() - start\n",
    "        print(\"run {}: {}\".format(i, t))\n",
    "    v2 = NB_DataHandler(amazon, imdb, yelp)\n",
    "    res = v2.test()\n",
    "    vals.append(res)\n",
    "    \n",
    "\n",
    "avg = sum(vals)/len(vals)\n",
    "finish = time.time()-start\n",
    "print(\"accuracy after {} runs:{} avg:{} \\ntime: {}s\"\n",
    "      .format(runs, round(max(vals),3), round(avg, 3), round(finish,3)))\n",
    "\n",
    "plt.plot(vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find common words\n",
    "\n",
    "voc = v2.vocabulary\n",
    "\n",
    "for w, fr in voc.items():\n",
    "    if len(fr['class']) > 1:\n",
    "        if sum(fr['frequency'].values()) > 10:\n",
    "            tot = sum(fr['frequency'].values())\n",
    "            perc = []\n",
    "            for c, f in fr['frequency'].items():\n",
    "                perc.append((f/tot)*100)\n",
    "            dif = abs(perc[0]-perc[1])\n",
    "            if dif < 15:\n",
    "                print ('{} -> {} -> {}'.format(w, perc, dif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print vocabulary\n",
    "\n",
    "voc = v2.vocabulary\n",
    "for word, data in voc.items():\n",
    "    print('{} -> {}'.format(word, data['frequency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = v2.training_data\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = v2.count_class()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.voc_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.log_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 17\n",
    "#res = v2.nb_classifier(v2.test_data[doc][1])\n",
    "#res = v2.test()\n",
    "#print(res)\n",
    "print(v2.test_data[doc][1])\n",
    "for word in v2.test_data[doc][1]:\n",
    "    if word in v2.vocabulary:\n",
    "        print('{} -> {}'.format(word, True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "c = {0:1234, 1:344}\n",
    "r = max(zip(c.keys(), c.values()))\n",
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.vocabulary['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = v2.test_data[17][1]\n",
    "\n",
    "sum_c = {}\n",
    "for cls, lp in v2.log_priors.items():\n",
    "    print('{} -> {}'.format(cls, lp))\n",
    "    sum_c[cls] = lp\n",
    "    for word in doc:\n",
    "        if word in v2.vocabulary:\n",
    "            print('{} -> {}'.format(word, v2.vocabulary[word]['probability'][cls]))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells_labelled.txt finished on line 1000 with 10202 words\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = v2.unit_test(amazon)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
